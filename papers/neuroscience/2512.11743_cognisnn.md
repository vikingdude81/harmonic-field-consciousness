# CogniSNN: Spiking Neural Networks with Random Graph Architectures

**arXiv:2512.11743**  
**Integration Date**: December 2025

## Paper Summary

CogniSNN presents a biologically-inspired spiking neural network architecture with three key innovations: neuron-expandability (dynamic growth), pathway-reusability (efficient learning without forgetting), and dynamic-configurability (adaptive structure). The use of random graph architectures mimics brain connectivity patterns.

### Core Concepts

1. **Neuron-Expandability**
   - Network can grow dynamically during learning
   - Add new neurons to capture new patterns
   - Mimics neurogenesis in biological brains
   - Enables continual learning

2. **Pathway-Reusability**
   - Key Pathway-based Learning without Forgetting (KP-LwF)
   - Reuse existing neural pathways for new tasks
   - Efficient knowledge transfer
   - Prevents catastrophic forgetting

3. **Dynamic-Configurability**
   - Adaptive network structure
   - Connections strengthen/weaken based on activity
   - Pruning of unused pathways
   - Self-organizing optimization

4. **Random Graph Architectures**
   - Small-world and scale-free connectivity
   - Biologically realistic connection patterns
   - Efficient information routing
   - Robust to perturbations

## Integration with Harmonic Field Consciousness

### Conceptual Mapping

| CogniSNN Concept | Consciousness Model Analog |
|-----------------|---------------------------|
| Spiking neurons | Harmonic oscillators |
| Spike trains | Mode amplitude dynamics |
| Network growth | Emergence of new modes |
| Pathway reuse | State transition mechanisms |
| Random graph | Brain connectome structure |
| Firing rates | Mode power distribution |

### Technical Implementation

#### 1. LIF Neuron Model

Replace classical oscillators with spiking neurons:

```python
from src.snn import LIFNeuron, LIFNetwork

# Single neuron
neuron = LIFNeuron(tau_m=20.0, v_threshold=1.0)

# Simulate
spike_train = []
for t in range(1000):
    I_input = np.sin(2 * np.pi * t / 100)  # Oscillatory input
    spiked = neuron.update(I_input)
    spike_train.append(spiked)

# Network of neurons
network = LIFNetwork(n_neurons=100, connectivity=adjacency_matrix)
spike_matrix, spike_trains = network.simulate(external_input, n_steps=1000)
```

#### 2. Spike Encoding

Convert harmonic amplitudes to/from spike trains:

```python
from src.snn import SpikeEncoder

# Rate coding: amplitude → firing rate
encoder = SpikeEncoder(encoding='rate')
spike_trains = encoder.encode(harmonic_amplitudes, duration=100.0)

# Temporal coding: amplitude → spike timing
encoder = SpikeEncoder(encoding='temporal')
spike_trains = encoder.encode(harmonic_amplitudes, duration=100.0)

# Decode back to amplitudes
decoded_amplitudes = encoder.decode(spike_trains)
```

#### 3. Consciousness Metrics from Spikes

Compute all metrics directly from spike trains:

```python
from src.snn import compute_spike_metrics

# Spike trains from network: (n_neurons, n_timesteps)
spike_matrix = network_simulation(...)

# Compute all consciousness metrics
metrics = compute_spike_metrics(spike_matrix, dt=0.1)

print(f"H_mode: {metrics['H_mode']:.3f}")
print(f"PR: {metrics['PR']:.3f}")
print(f"R (phase coherence): {metrics['R']:.3f}")
print(f"S_dot (entropy rate): {metrics['S_dot']:.3f}")
print(f"kappa (criticality): {metrics['kappa']:.3f}")
print(f"C_t (overall): {metrics['C_t']:.3f}")
```

Key adaptations:
- **H_mode**: Entropy of firing rate distribution
- **PR**: Participation ratio of active neurons
- **R**: Spike synchrony across neurons
- **Ṡ**: Inter-spike interval variability
- **κ**: Avalanche statistics (power-law exponent)

#### 4. Dynamic Network Growth

Start small and grow based on learning:

```python
# Start with small network
network = LIFNetwork(n_neurons=100, connectivity=initial_graph)

# Training loop
for epoch in range(n_epochs):
    # Simulate with current network
    spike_matrix = network.simulate(training_data[epoch])
    metrics = compute_spike_metrics(spike_matrix)
    
    # Expand if performance plateaus
    if should_expand(metrics):
        # Add 10% more neurons
        n_new = int(network.n_neurons * 0.1)
        network = expand_network(network, n_new)
        
        print(f"Expanded to {network.n_neurons} neurons")
    
    # Track consciousness emergence
    consciousness_trajectory.append(metrics['C_t'])
```

#### 5. Pathway Reusability

Reuse pathways for state transitions:

```python
# Learn wake → sleep transition
wake_spikes = simulate_wake_state(network)
sleep_spikes = simulate_sleep_state(network)

# Identify key pathways active during transition
key_pathways = identify_transition_pathways(wake_spikes, sleep_spikes)

# Reuse for other transitions (e.g., wake → anesthesia)
# Without retraining entire network
efficient_transition = reuse_pathways(network, key_pathways, target='anesthesia')
```

## Experiments

### Experiment 1: SNN Harmonic Oscillators
- **File**: `experiments/category8_spiking_consciousness/exp1_snn_harmonic_oscillators.py`
- **Goal**: Replace classical oscillators with spiking neurons
- **Setup**: LIF network with connectome-based connectivity
- **Metrics**: All consciousness metrics from spike trains
- **Expected**: SNN metrics match classical within 10%

### Experiment 2: Pathway Reuse for Transitions
- **File**: `experiments/category8_spiking_consciousness/exp2_pathway_reuse_transitions.py`
- **Goal**: Efficient state transitions via pathway reuse
- **Setup**: Learn wake ↔ sleep, test on other transitions
- **Metrics**: Transition efficiency, forgetting rate
- **Expected**: 50%+ faster learning with pathway reuse

### Experiment 3: Dynamic Network Growth
- **File**: `experiments/category8_spiking_consciousness/exp3_dynamic_network_growth.py`
- **Goal**: Track consciousness emergence during growth
- **Setup**: Start with 100 neurons, grow to 500
- **Metrics**: C(t) trajectory, final performance
- **Expected**: 20%+ performance improvement with growth

### Experiment 4: Neuromorphic Deployment
- **File**: `experiments/category8_spiking_consciousness/exp4_neuromorphic_deployment.py`
- **Goal**: Export to neuromorphic hardware format
- **Setup**: Convert to SpiNNaker/Loihi format
- **Metrics**: Power consumption, real-time latency
- **Expected**: <10W power, <10ms latency

## Integration Points

### Connection to Network Topology

Leverage random graph generators from category1:

```python
from experiments.utils import graph_generators as gg

# Small-world connectivity (like brain)
G = gg.generate_small_world(N=500, k_neighbors=20, rewiring_prob=0.1)
adjacency = nx.to_numpy_array(G)

# Scale-free connectivity (hub structure)
G = gg.generate_scale_free(N=500, m_edges=10)
adjacency = nx.to_numpy_array(G)

# Create SNN with brain-like connectivity
network = LIFNetwork(n_neurons=500, connectivity=adjacency)
```

### Biological Realism

SNNs provide biological plausibility:

1. **Temporal Dynamics**: Actual spiking instead of continuous signals
2. **Event-Driven**: Sparse, efficient computation
3. **Local Learning**: Spike-timing-dependent plasticity (STDP)
4. **Energy Efficiency**: Only active when spiking

### Hardware Deployment

Export to neuromorphic chips:

```python
def export_to_spinnaker(network):
    """Convert LIF network to SpiNNaker format."""
    # SpiNNaker configuration
    config = {
        'neurons': network.n_neurons,
        'tau_m': [n.tau_m for n in network.neurons],
        'v_threshold': [n.v_threshold for n in network.neurons],
        'connectivity': network.connectivity,
        'weights': network.connectivity * synaptic_weight
    }
    return config

def export_to_loihi(network):
    """Convert LIF network to Intel Loihi format."""
    # Loihi configuration
    ...
```

## Success Criteria

- [x] Core SNN modules implemented (LIF, spike encoding, metrics)
- [ ] SNN metrics match classical metrics within 10%
- [ ] Dynamic growth improves performance by 20%+
- [ ] Pathway reuse accelerates learning by 50%+
- [ ] Neuromorphic deployment feasible with <10W power
- [ ] Hardware latency <10ms for real-time consciousness tracking

## Future Enhancements

1. **Learning Rules**:
   - Spike-Timing-Dependent Plasticity (STDP)
   - Reward-modulated STDP
   - Homeostatic plasticity

2. **Advanced Neuron Models**:
   - Izhikevich neurons (more biological)
   - Hodgkin-Huxley neurons (most realistic)
   - Adaptive exponential integrate-and-fire

3. **Network Architectures**:
   - Recurrent connections (memory)
   - Hierarchical structure (cortical layers)
   - Inhibitory neurons (E/I balance)

4. **Neuromorphic Hardware**:
   - SpiNNaker deployment
   - Intel Loihi integration
   - IBM TrueNorth compatibility

5. **Real-Time Applications**:
   - Brain-computer interfaces
   - Neuroprosthetics
   - Closed-loop neurofeedback

## Implementation Notes

### Library Options

For production use, consider integrating established SNN libraries:

1. **Norse** (PyTorch-based):
```python
import norse.torch as norse

# Norse LIF neurons
lif = norse.LIFParameters(tau_mem_inv=1/20.0, v_th=1.0)
network = norse.LIFNetwork(lif, n_neurons=100)
```

2. **BindsNET** (flexible):
```python
from bindsnet.network import Network
from bindsnet.network.nodes import LIFNodes

network = Network()
layer = LIFNodes(n=100, tau=20.0, thresh=1.0)
network.add_layer(layer, name='neurons')
```

3. **Brian2** (research-oriented):
```python
from brian2 import *

# Brian2 LIF model
eqs = '''
dv/dt = (-(v-v_rest) + I)/tau : volt
I : amp
'''
neurons = NeuronGroup(100, eqs, threshold='v>v_th', reset='v=v_reset')
```

### Current Implementation
Our implementation provides:
- ✅ Basic LIF neuron dynamics
- ✅ Network simulation with connectivity
- ✅ Spike encoding/decoding
- ✅ Consciousness metrics from spikes
- ✅ Modular, easy-to-extend design

## References

- Original Paper: arXiv:2512.11743
- Related: Neuromorphic computing, spiking neural networks, brain-inspired AI
- Hardware: SpiNNaker, Intel Loihi, IBM TrueNorth
- Libraries: Norse, BindsNET, Brian2, NEST
