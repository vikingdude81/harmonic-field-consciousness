# Brain Scale Informed Spatiotemporal Representation (BaRISTA)

**arXiv:2512.12135**  
**Integration Date**: December 2025

## Paper Summary

BaRISTA introduces a spatiotemporal transformer architecture specifically designed for brain activity analysis. Instead of treating individual channels/sensors as tokens, it uses region-level encoding with flexible spatial scales, enabling more interpretable and efficient brain state modeling.

### Core Concepts

1. **Region-Level Encoding**
   - Brain modules/regions as tokens (not individual nodes)
   - Hierarchical spatial organization
   - More biologically meaningful than node-level
   - Reduces computational complexity

2. **Spatiotemporal Transformer**
   - Self-attention over brain regions
   - Captures both spatial and temporal dependencies
   - Multi-head attention for different aspects
   - Attention weights show region importance

3. **Masked Reconstruction Learning**
   - Self-supervised pre-training strategy
   - Mask 15% of regions, predict their activity
   - Learns robust representations
   - Improves downstream task performance

4. **Flexible Spatial Scales**
   - Can analyze at multiple granularities:
     - Individual nodes
     - Communities/modules
     - Hemispheres
     - Whole brain
   - Find optimal scale for each task

## Integration with Harmonic Field Consciousness

### Conceptual Mapping

| BaRISTA Concept | Consciousness Model Analog |
|----------------|---------------------------|
| Brain regions | Modular network communities |
| Region tokens | Community-level harmonic activity |
| Spatial attention | Inter-module integration |
| Temporal attention | Mode dynamics evolution |
| Masked reconstruction | Robust metric estimation |
| Spatial scales | Hierarchical brain organization |

### Technical Implementation

#### 1. Region-Level Encoding

Encode communities as tokens instead of individual nodes:

```python
from src.transformers import BaRISTAModel

# Create model with 8 brain regions
model = BaRISTAModel(n_regions=8, n_features=32, n_heads=4)

# Region data: activity of each module
region_data = np.random.randn(8, 100)  # 8 regions, 100 timepoints

# Encode and analyze
outputs = model.forward(region_data)
tokens = outputs['tokens']  # (n_regions, n_features)
attention = outputs['attention']  # (n_heads, n_regions, n_regions)
```

#### 2. Attention Analysis

Attention weights reveal which regions interact for consciousness:

```python
from src.transformers import AttentionVisualizer

visualizer = AttentionVisualizer(region_names=['V1', 'V2', 'PFC', 'Thal', ...])

# Plot attention matrix
fig = visualizer.plot_attention_matrix(
    attention, 
    title="Inter-Region Attention for Wake State",
    save_path="results/wake_attention.png"
)

# Plot attention flow
fig = visualizer.plot_attention_flow(
    attention,
    top_k=3,  # Show top 3 connections per region
    title="Critical Connections for Consciousness"
)

# Attention entropy (focus vs distributed)
fig = visualizer.plot_attention_entropy(
    attention,
    title="Attention Diversity Across Regions"
)
```

#### 3. Consciousness State Prediction

Use transformer to predict consciousness states:

```python
# Predict state from region activities
prediction = model.predict_consciousness_state(region_data)

print(f"State: {prediction['state']}")  # 'wake', 'nrem', 'rem', 'anesthesia'
print(f"Confidence: {prediction['C_score']:.3f}")
print(f"H_mode: {prediction['H_mode']:.3f}")
print(f"PR: {prediction['PR']:.3f}")

# Attention shows important regions
important_regions = np.mean(prediction['attention'], axis=(0, 2))  # Average attention received
top_regions = np.argsort(important_regions)[-3:]
print(f"Top regions for this state: {top_regions}")
```

#### 4. Masked Reconstruction

Self-supervised learning with masked region reconstruction:

```python
# Mask 15% of regions
masked_data, reconstructed = model.masked_reconstruction(
    region_data, 
    mask_ratio=0.15
)

# Evaluate reconstruction quality
reconstruction_error = np.mean((region_data - reconstructed)**2)

# Good reconstruction indicates model learned meaningful representations
```

## Experiments

### Experiment 1: Region-Level Encoding
- **File**: `experiments/category7_spatiotemporal_transformers/exp1_region_level_encoding.py`
- **Goal**: Compare region-level vs node-level encoding
- **Setup**: Modular networks with 2, 4, 6, 8 communities
- **Metrics**: Classification accuracy, computational time
- **Expected**: Region-level outperforms node-level, especially on larger networks

### Experiment 2: Masked Reconstruction
- **File**: `experiments/category7_spatiotemporal_transformers/exp2_masked_reconstruction.py`
- **Goal**: Self-supervised learning improves representations
- **Setup**: Train on wake/sleep/anesthesia data with masking
- **Metrics**: Reconstruction error, downstream classification accuracy
- **Expected**: Masked pre-training improves by 10%+

### Experiment 3: Transformer Consciousness Prediction
- **File**: `experiments/category7_spatiotemporal_transformers/exp3_transformer_consciousness.py`
- **Goal**: Full transformer pipeline for C(t) prediction
- **Setup**: Train transformer on labeled consciousness states
- **Metrics**: Classification accuracy, attention interpretability
- **Expected**: >85% accuracy, attention highlights known critical regions

### Experiment 4: Spatial Scale Analysis
- **File**: `experiments/category7_spatiotemporal_transformers/exp4_spatial_scale_analysis.py`
- **Goal**: Find optimal spatial granularity for each metric
- **Setup**: Test scales from nodes → communities → hemispheres
- **Metrics**: Metric accuracy vs computational cost
- **Expected**: Intermediate scales (communities) optimal for most metrics

## Integration Points

### Connection to Modular Networks

Leverages existing modular network experiments:

```python
from experiments.utils import graph_generators as gg

# Generate modular network (from category1/exp4)
G, communities = gg.generate_modular(N=100, n_modules=8, 
                                     intra_prob=0.3, inter_prob=0.05)

# Compute region-level activity
region_activities = []
for community in communities:
    # Activity of nodes in this community
    community_activity = node_activities[list(community)]
    region_activities.append(np.mean(community_activity, axis=0))

region_data = np.array(region_activities)

# Analyze with BaRISTA
model = BaRISTAModel(n_regions=len(communities))
prediction = model.predict_consciousness_state(region_data)
```

### New Utilities

**utils/transformer_utils.py**:
- Token preparation from node/region data
- Attention pattern analysis
- Hierarchical region grouping
- Spatial scale optimization

## Success Criteria

- [x] Core transformer modules implemented
- [ ] Region-level encoding outperforms node-level
- [ ] Transformer achieves >85% state classification accuracy
- [ ] Attention weights highlight interpretable brain regions
- [ ] Masked reconstruction improves performance by 10%+
- [ ] Optimal spatial scale identified for each metric

## Future Enhancements

1. **Full PyTorch Implementation**: Current implementation is simplified
   - Replace with full transformer architecture
   - Add positional encoding for temporal structure
   - Learnable parameters via backpropagation

2. **Temporal Dynamics**: Explicitly model time
   - Recurrent attention over time windows
   - Temporal positional encodings
   - Predict future consciousness states

3. **Multi-Modal Integration**: Combine different data types
   - EEG + fMRI + behavior
   - Cross-modal attention
   - Unified spatiotemporal representation

4. **Interpretability Tools**:
   - Attention rollout visualization
   - Region contribution analysis
   - Causal intervention studies

5. **Transfer Learning**:
   - Pre-train on large datasets
   - Fine-tune on specific tasks/subjects
   - Few-shot consciousness state detection

## Implementation Notes

### Current Status
The current implementation provides:
- ✅ Basic multi-head attention mechanism
- ✅ Region-level token encoding
- ✅ Attention visualization tools
- ✅ Consciousness state prediction
- ✅ Masked reconstruction framework

### Limitations
- Simplified attention (not learned from data)
- No temporal positional encoding
- No training/optimization loop
- Fixed attention weights

### Full Implementation (Optional)
For production use, consider adding:
```python
import torch
import torch.nn as nn

class FullBaRISTA(nn.Module):
    def __init__(self, n_regions, d_model, n_heads, n_layers):
        super().__init__()
        self.embedding = nn.Linear(n_features, d_model)
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model, n_heads),
            num_layers=n_layers
        )
        self.classifier = nn.Linear(d_model, n_classes)
    
    def forward(self, region_data):
        # Full learnable transformer
        ...
```

## References

- Original Paper: arXiv:2512.12135
- Related: Vision Transformers (ViT), BERT for brain signals
- Applications: Clinical diagnosis, BCI, neuroscience research
